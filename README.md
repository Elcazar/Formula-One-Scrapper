# Proyecto: AnÃ¡lisis de Datos F1

Este proyecto contiene un conjunto de herramientas organizadas en varios mÃ³dulos que permiten extraer y tratar datos de carreras de FÃ³rmula 1, preparando informaciÃ³n de eventos de carreras para su posterior anÃ¡lisis, obtenidos desde mÃºltiples fuentes.


# ğŸ–‹ï¸ Authors  

<p align="left">
  <a href="https://github.com/Elcazar">
    <img src="https://img.shields.io/badge/github-181717?style=for-the-badge&logo=github&logoColor=white" alt="GitHub"/>
  </a>
  <span style="margin-left: 10px;"> Alejandro AlcÃ¡zar Mendoza</span>
</p>


<p align="left">
  <a href="https://github.com/SergioFdz05">
    <img src="https://img.shields.io/badge/github-181717?style=for-the-badge&logo=github&logoColor=white" alt="GitHub"/>
  </a>
  <span style="margin-left: 10px;"> Leyre Fontaneda FernÃ¡ndez</span>
</p>


<p align="left">
  <a href="https://github.com/LuisGV10">
    <img src="https://img.shields.io/badge/github-181717?style=for-the-badge&logo=github&logoColor=white" alt="GitHub"/>
  </a>
  <span style="margin-left: 10px;"> Ana Ling Gil GonzÃ¡lez</span>
</p>


<p align="left">
  <a href="https://github.com/yagocastillo126">
    <img src="https://img.shields.io/badge/github-181717?style=for-the-badge&logo=github&logoColor=white" alt="GitHub"/>
  </a>
  <span style="margin-left: 10px;"> Raynel Antonio GarcÃ­a Bryan</span>
</p>
## IntroducciÃ³n

El proyecto proporciona funciones para:

- Cargar datos desde la pÃ¡gina web de Wikipedida mediante web scraping.
- Cargar datos usando la API Jolpica FI.
- Limpiar y unir todos los datos en un unico archivo.

La organizaciÃ³n modular permite una fÃ¡cil escalabilidad y mantenimiento.

## Requisitos

- **Python:** 3.11.4
- **Pandas:** 1.5.3
- **NumPy:** 1.24.3
- **Regex:** 2.5.116
- **Scrapy:** 2.8.0
- **Urllib3:** 1.26.16


## EjecuciÃ³n

Para ejecutar el el proyecto:
1. Configura el entorno e instala las dependencias.
2. Ejecuta el script principal:
   ```
   python main.py
   ```

## Estructura del Proyecto

â”œâ”€â”€ data                       # Carpeta con los datos de las carreras en formato CSV.
â”‚   â”œâ”€â”€ 2012                   # Subcarpeta con datos de 2012.
â”‚   â”œâ”€â”€ 2013                   # Subcarpeta con datos de 2013.
â”‚   â””â”€â”€ ...
â”œâ”€â”€ docs                       # Carpeta para la documentaciÃ³n adicional.
â”‚   â”œâ”€â”€ conclusion.pdf         # Documento con las conclusiones del proyecto.
â”‚   â””â”€â”€ graficas.pdf           # Documento con las grÃ¡ficas generadas durante el proyecto.
â”œâ”€â”€ modules                    # CÃ³digo fuente del proyecto.
â”‚   â”œâ”€â”€ data_cleaning.py       # MÃ³dulo para limpieza de datos.
â”‚   â”œâ”€â”€ crawler.py             # MÃ³dulo para obtenciÃ³n de datos de la pÃ¡gina Wikipedia 
â”‚   â”œâ”€â”€ jolpica.py             # MÃ³dulo para obtenciÃ³n de informaciÃ³n de la API Jolpica FI.
â”‚   â””â”€â”€ merge.py               # MÃ³dulo para la uniÃ³n de los dataframes.
â”œâ”€â”€ results                    # Carpeta con datos en formato CSV y json.
â”‚   â”œâ”€â”€ crawler_output.json    # Output del crawler.py en formato JSON.
â”‚   â”œâ”€â”€ drivers.csv            # Output de jolpica.py en formato csv.
â”‚   â””â”€â”€ merged.csv             # Resultado final del proyecto
â”œâ”€â”€ main.py                    # Punto de entrada principal del proyecto.
â””â”€â”€ README.md                  # DocumentaciÃ³n del proyecto.


## DescripciÃ³n de los MÃ³dulos

### crawler.py

Incluye funciones para realizar web scraping de la pÃ¡gina web de Wikipedia de las carreras en las distintas temporadas de Formula 1 de 2012 hasta 2023. 
    - La funciÃ³n principal es start_crawler
    - Guarda los datos de las carreras dentro de una carpera "data"
    - Guarda un fichero crawler_output.json en la carpeta de "results" generado con scrapy

### jolpica.py
Mediante las funciones de este mÃ³dulo se obtiene informaciÃ³n de la API Jolpica FI, relativa a los pit-stops de las distintas temporadas de Formula 1
    - La funciÃ³n get_drivers_mapping debe ejecutarse primero para obtener el mapping entre ID y nÃºmero. Este mapping se guarda en la carpeta de "results"
    - La funciÃ³n get_pit_stops guarda para cada aÃ±o desde 2019 hasta 2024 un df a partir del 
    endpoint de pitstops un df conteniendo â€œDriverIdâ€, â€œDriverNumberâ€, â€œNPitstopsâ€ â€œMedianPitStopDurationâ€ en la carpeta de "data".

### merge.py
El mÃ³dulo de merge se encarga de obter un Ãºnico df a partir de todos los datos anteriores. Emplea un mÃ³dulo auxiliar, data_cleaning.py, para procesar primero los datos.
	- La funciÃ³n merge_API_WIKI se encarga de hacer el merge para cada par de dataframes de una carrera en un aÃ±o concreto


### data_cleaning.py
Este mÃ³dulo contiene funciones para limpiar y procesar los datos crudos de las carreras. 
    - La funciÃ³n principal es clean_dfs, que se emplea en merge.py para limpiar los datos antes de hacer el merge. 
    - La funciÃ³n clean_API_df limpia el DataFrame obtenido de la API. Actualiza los nÃºmeros de los pilotos segÃºn un diccionario de cambios especificado por aÃ±o.
    - La funciÃ³n clean_WIKI_df limpia el DataFrame obtenido de Wikipedia. Convierte los tiempos de los pilotos en formato "hh:mm:ss.mmm" o "+X:XX.X" a segundos decimales para un anÃ¡lisis mÃ¡s sencillo, y maneja valores especiales como "+ Laps" o "Collision". TambiÃ©n maneja casos especiales como los puntos de vuelta rÃ¡pida en la columna Points. AdemÃ¡s, se eliminan anotaciones de Wikipedia en las columnas de Points y Positions.
    - La funciÃ³n time_to_decimal convierte un tiempo en formato de texto a segundos decimales. Si el tiempo es un valor relativo (por ejemplo, "+X:XX.X"), lo ajusta con respecto al primer piloto, y maneja casos como el retiro.

### main.py
	- Script para ejecutar

### Ãšltima actualizaciÃ³n: Diciembre 2024
